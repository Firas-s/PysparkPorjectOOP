"30-May-23 16:48:48" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 16:48:48" - root - INFO - main() is started ...
"30-May-23 16:49:09" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 16:49:09" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 16:49:09" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.name 'os' is not defined
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 19, in main
    data_ingestor = DataIngestor(spark, gav.csv_directory, gav.parquet_directory, gav.processed_files_directory)
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\ingestion.py", line 18, in __init__
    self.processed_files_path = os.path.join(self.processed_files_directory, "processed_files.txt")
NameError: name 'os' is not defined
"30-May-23 16:51:13" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 16:51:13" - root - INFO - main() is started ...
"30-May-23 16:51:32" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 16:51:32" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 16:51:32" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.name 'os' is not defined
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 19, in main
    data_ingestor = DataIngestor(spark, gav.csv_directory, gav.parquet_directory, gav.processed_files_directory)
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\ingestion.py", line 18, in __init__
    self.processed_files_path = os.path.join(self.processed_files_directory, "processed_files.txt")
NameError: name 'os' is not defined
"30-May-23 17:07:08" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:07:08" - root - INFO - main() is started ...
"30-May-23 17:07:27" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:07:27" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:07:27" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.[WinError 3] Le chemin d’accès spécifié est introuvable: 'C:\\Users\\fabassi\\PycharmProjects\\PysparkPorjectOOP\\src\\python\\bin\\staging\\dimension_city'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 25, in main
    data_ingestor.ingest_data()
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\ingestion.py", line 32, in ingest_data
    for csv_file in os.listdir(self.csv_directory):
FileNotFoundError: [WinError 3] Le chemin d’accès spécifié est introuvable: 'C:\\Users\\fabassi\\PycharmProjects\\PysparkPorjectOOP\\src\\python\\bin\\staging\\dimension_city'
"30-May-23 17:11:11" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:11:11" - root - INFO - main() is started ...
"30-May-23 17:11:38" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:11:38" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:11:38" - root - INFO - Le fichier us_cities_dimension.parquet a déjà été traité. Ignoré.
"30-May-23 17:11:38" - root - INFO - presc_run_pipeline.py is Completed.
"30-May-23 17:13:03" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:13:03" - root - INFO - main() is started ...
"30-May-23 17:13:22" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:13:22" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:13:34" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.[Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python\\processed_files.txt'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 25, in main
    data_ingestor.ingest_data()
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\ingestion.py", line 62, in ingest_data
    with open(self.processed_files_path, 'w') as file:
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python\\processed_files.txt'
"30-May-23 17:13:44" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:13:44" - root - INFO - main() is started ...
"30-May-23 17:14:05" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:14:05" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:14:18" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.[Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python\\processed_files.txt'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 25, in main
    data_ingestor.ingest_data()
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\ingestion.py", line 62, in ingest_data
    with open(self.processed_files_path, 'w') as file:
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python\\processed_files.txt'
"30-May-23 17:15:37" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:15:37" - root - INFO - main() is started ...
"30-May-23 17:16:06" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:16:06" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:16:21" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.[Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python/processed.txt\\processed_files.txt'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 25, in main
    data_ingestor.ingest_data()
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\ingestion.py", line 62, in ingest_data
    with open(self.processed_files_path, 'w') as file:
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python/processed.txt\\processed_files.txt'
"30-May-23 17:17:14" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:17:14" - root - INFO - main() is started ...
"30-May-23 17:17:35" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:17:35" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:17:49" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.[Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python/processed.txt\\processed_files.txt'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 25, in main
    data_ingestor.ingest_data()
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\ingestion.py", line 62, in ingest_data
    with open(self.processed_files_path, 'w') as file:
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/main/python/processed.txt\\processed_files.txt'
"30-May-23 17:22:22" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:22:22" - root - INFO - main() is started ...
"30-May-23 17:22:46" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:22:46" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:23:00" - root - INFO - presc_run_pipeline.py is Completed.
"30-May-23 17:23:58" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:23:58" - root - INFO - main() is started ...
"30-May-23 17:24:22" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:24:22" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:24:34" - root - INFO - presc_run_pipeline.py is Completed.
"30-May-23 17:26:48" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:26:48" - root - INFO - main() is started ...
"30-May-23 17:27:06" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:27:06" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:27:18" - root - INFO - presc_run_pipeline.py is Completed.
"30-May-23 17:27:31" - root - INFO - run_presc_pipeline is Started ...
"30-May-23 17:27:31" - root - INFO - main() is started ...
"30-May-23 17:27:53" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:27:53" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:27:53" - root - INFO - Le fichier us_cities_dimension.parquet a déjà été traité. Ignoré.
"30-May-23 17:27:53" - root - INFO - presc_run_pipeline.py is Completed.
"30-May-23 17:41:51" - root - INFO - pipeline is Started ...
"30-May-23 17:41:51" - root - INFO - main() is started ...
"30-May-23 17:42:19" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 5, 30))]
"30-May-23 17:42:19" - validations - INFO - Spark object is validated. Spark Object is ready.
"30-May-23 17:42:19" - __main__ - INFO - Maître Spark : local
"30-May-23 17:42:19" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"30-May-23 17:42:19" - root - INFO - Le fichier test.csv a déjà été traité. Ignoré.
"30-May-23 17:42:19" - root - INFO - Le fichier USA_Presc_Medicare_Data_12021.csv a déjà été traité. Ignoré.
"30-May-23 17:42:19" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"30-May-23 17:42:19" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"30-May-23 17:42:19" - root - INFO - presc_run_pipeline.py is Completed.
"01-Jun-23 12:21:17" - root - INFO - pipeline is Started ...
"01-Jun-23 12:21:17" - root - INFO - main() is started ...
"01-Jun-23 12:22:50" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 1))]
"01-Jun-23 12:22:50" - validations - INFO - Spark object is validated. Spark Object is ready.
"01-Jun-23 12:22:50" - __main__ - INFO - Maître Spark : local
"01-Jun-23 12:22:50" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"01-Jun-23 12:22:50" - root - INFO - Le fichier test.csv a déjà été traité. Ignoré.
"01-Jun-23 12:22:50" - root - INFO - Le fichier USA_Presc_Medicare_Data_12021.csv a déjà été traité. Ignoré.
"01-Jun-23 12:22:50" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"01-Jun-23 12:22:50" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"01-Jun-23 12:22:50" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 10:27:22" - root - INFO - pipeline is Started ...
"05-Jun-23 10:27:22" - root - INFO - main() is started ...
"05-Jun-23 10:27:55" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 10:27:55" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 10:27:55" - __main__ - INFO - Maître Spark : local
"05-Jun-23 10:27:56" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 10:27:56" - root - INFO - Le fichier test.csv a déjà été traité. Ignoré.
"05-Jun-23 10:27:56" - root - INFO - Le fichier USA_Presc_Medicare_Data_12021.csv a déjà été traité. Ignoré.
"05-Jun-23 10:27:56" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"05-Jun-23 10:27:56" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"05-Jun-23 10:27:56" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 13:43:05" - root - INFO - pipeline is Started ...
"05-Jun-23 13:43:05" - root - INFO - main() is started ...
"05-Jun-23 13:43:33" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 13:43:33" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 13:43:33" - __main__ - INFO - Maître Spark : local
"05-Jun-23 13:43:33" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 13:43:49" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 13:46:32" - root - INFO - pipeline is Started ...
"05-Jun-23 13:46:32" - root - INFO - main() is started ...
"05-Jun-23 13:46:52" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 13:46:52" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 13:46:52" - __main__ - INFO - Maître Spark : local
"05-Jun-23 13:46:52" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 13:46:52" - root - INFO - Le fichier test.csv a déjà été traité. Ignoré.
"05-Jun-23 13:46:52" - root - INFO - Le fichier USA_Presc_Medicare_Data_12021.csv a déjà été traité. Ignoré.
"05-Jun-23 13:46:52" - root - INFO - The file test.csv has already been treated. Ignored
"05-Jun-23 13:46:52" - root - INFO - The file test.csv has already been treated. Ignored
"05-Jun-23 13:46:52" - root - INFO - The file USA_Presc_Medicare_Data_12021.csv has already been treated. Ignored
"05-Jun-23 13:46:52" - root - INFO - The file USA_Presc_Medicare_Data_12021.csv has already been treated. Ignored
"05-Jun-23 13:46:52" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 13:47:45" - root - INFO - pipeline is Started ...
"05-Jun-23 13:47:45" - root - INFO - main() is started ...
"05-Jun-23 13:48:02" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 13:48:02" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 13:48:02" - __main__ - INFO - Maître Spark : local
"05-Jun-23 13:48:02" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 13:48:13" - root - INFO - The file test.csv has already been treated. Ignored
"05-Jun-23 13:48:13" - root - INFO - The file test.csv has already been treated. Ignored
"05-Jun-23 13:48:13" - root - INFO - The file USA_Presc_Medicare_Data_12021.csv has already been treated. Ignored
"05-Jun-23 13:48:13" - root - INFO - The file USA_Presc_Medicare_Data_12021.csv has already been treated. Ignored
"05-Jun-23 13:48:13" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 13:52:35" - root - INFO - pipeline is Started ...
"05-Jun-23 13:52:35" - root - INFO - main() is started ...
"05-Jun-23 13:52:58" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 13:52:58" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 13:52:58" - __main__ - INFO - Maître Spark : local
"05-Jun-23 13:52:58" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 13:52:58" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 13:53:40" - root - INFO - pipeline is Started ...
"05-Jun-23 13:53:40" - root - INFO - main() is started ...
"05-Jun-23 13:53:59" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 13:53:59" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 13:53:59" - __main__ - INFO - Maître Spark : local
"05-Jun-23 13:53:59" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 13:53:59" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 13:54:15" - root - INFO - pipeline is Started ...
"05-Jun-23 13:54:15" - root - INFO - main() is started ...
"05-Jun-23 13:54:36" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 13:54:36" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 13:54:36" - __main__ - INFO - Maître Spark : local
"05-Jun-23 13:54:36" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 13:54:36" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 14:02:29" - root - INFO - pipeline is Started ...
"05-Jun-23 14:02:29" - root - INFO - main() is started ...
"05-Jun-23 14:02:56" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 14:02:56" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 14:02:56" - __main__ - INFO - Maître Spark : local
"05-Jun-23 14:02:56" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 14:02:59" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"05-Jun-23 14:02:59" - root - INFO - The file us_cities_dimension.parquet has already been treated. Ignored
"05-Jun-23 14:03:07" - root - INFO - The file test.csv has already been treated. Ignored
"05-Jun-23 14:03:07" - root - INFO - The file test.csv has already been treated. Ignored
"05-Jun-23 14:03:07" - root - INFO - The file USA_Presc_Medicare_Data_12021.csv has already been treated. Ignored
"05-Jun-23 14:03:07" - root - INFO - The file USA_Presc_Medicare_Data_12021.csv has already been treated. Ignored
"05-Jun-23 14:03:07" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 14:28:33" - root - INFO - pipeline is Started ...
"05-Jun-23 14:28:33" - root - INFO - main() is started ...
"05-Jun-23 14:28:54" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 14:28:54" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 14:28:54" - __main__ - INFO - Maître Spark : local
"05-Jun-23 14:28:54" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 14:29:07" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 14:34:53" - root - INFO - pipeline is Started ...
"05-Jun-23 14:34:53" - root - INFO - main() is started ...
"05-Jun-23 14:35:14" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 14:35:14" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 14:35:14" - __main__ - INFO - Maître Spark : local
"05-Jun-23 14:35:14" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 14:35:24" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 14:35:46" - root - INFO - pipeline is Started ...
"05-Jun-23 14:35:46" - root - INFO - main() is started ...
"05-Jun-23 14:36:10" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 14:36:10" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 14:36:10" - __main__ - INFO - Maître Spark : local
"05-Jun-23 14:36:10" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 14:36:25" - root - INFO - presc_run_pipeline.py is Completed.
"05-Jun-23 14:36:59" - root - INFO - pipeline is Started ...
"05-Jun-23 14:36:59" - root - INFO - main() is started ...
"05-Jun-23 14:37:20" - validations - INFO - Validate the Spark object by printing Current Date - [Row(current_date()=datetime.date(2023, 6, 5))]
"05-Jun-23 14:37:20" - validations - INFO - Spark object is validated. Spark Object is ready.
"05-Jun-23 14:37:20" - __main__ - INFO - Maître Spark : local
"05-Jun-23 14:37:20" - __main__ - INFO - Nombre d'exécuteurs Spark : 1
"05-Jun-23 14:37:22" - root - ERROR - Error Occured in the main() method. Please check the Stack Trace to go to the respective module and fix it.'tuple' object has no attribute 'show'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\pipeline.py", line 29, in main
    df_parquet.show()
AttributeError: 'tuple' object has no attribute 'show'
"05-Jun-23 15:06:06" - root - INFO - Main function has started...
"05-Jun-23 15:06:19" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:06:19" - root - INFO - pipeline is Started ...
"05-Jun-23 15:06:19" - root - INFO - Main function has started...
"05-Jun-23 15:06:19" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:06:49" - root - INFO - Main function has started...
"05-Jun-23 15:07:01" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:07:01" - root - INFO - pipeline is Started ...
"05-Jun-23 15:07:01" - root - INFO - Main function has started...
"05-Jun-23 15:07:01" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:07:23" - root - INFO - Main function has started...
"05-Jun-23 15:07:34" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:07:34" - root - INFO - pipeline is Started ...
"05-Jun-23 15:07:34" - root - INFO - Main function has started...
"05-Jun-23 15:07:34" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:09:54" - root - INFO - Main function has started...
"05-Jun-23 15:10:06" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:10:06" - root - INFO - pipeline is Started ...
"05-Jun-23 15:10:06" - root - INFO - Main function has started...
"05-Jun-23 15:10:06" - root - ERROR - An error occurred in the main function: name 'DataIngestion' is not defined
"05-Jun-23 15:10:14" - root - INFO - Main function has started...
"05-Jun-23 15:10:30" - root - ERROR - An error occurred in the main function: DataIngestor.__init__() missing 3 required positional arguments: 'csv_directory', 'parquet_directory', and 'processed_files_directory'
"05-Jun-23 15:10:30" - root - INFO - pipeline is Started ...
"05-Jun-23 15:10:30" - root - INFO - Main function has started...
"05-Jun-23 15:10:30" - root - ERROR - An error occurred in the main function: DataIngestor.__init__() missing 3 required positional arguments: 'csv_directory', 'parquet_directory', and 'processed_files_directory'
"05-Jun-23 15:47:42" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 15:47:42" - __main__ - INFO - Main function has started...
"05-Jun-23 15:48:01" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:48:10" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:48:10" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:48:12" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:48:12" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 15:48:44" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 15:48:44" - __main__ - INFO - Main function has started...
"05-Jun-23 15:48:54" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:48:54" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:48:54" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:48:54" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:48:54" - __main__ - INFO - Main function has completed.
"05-Jun-23 15:51:02" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 15:51:02" - __main__ - INFO - Main function has started...
"05-Jun-23 15:51:17" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:51:17" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:51:17" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:51:17" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:51:17" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 15:52:41" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 15:52:41" - __main__ - INFO - Main function has started...
"05-Jun-23 15:52:53" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:52:53" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:52:53" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:52:53" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:52:53" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 15:54:02" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 15:54:02" - __main__ - INFO - Main function has started...
"05-Jun-23 15:54:14" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:54:14" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:54:14" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:54:14" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:54:14" - __main__ - INFO - Main function has completed.
"05-Jun-23 15:54:35" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 15:54:35" - __main__ - INFO - Main function has started...
"05-Jun-23 15:54:48" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:54:57" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:54:57" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 15:54:58" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 15:55:01" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'show'
"05-Jun-23 15:58:55" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 15:58:55" - __main__ - INFO - Main function has started...
"05-Jun-23 15:59:08" - __main__ - ERROR - An error occurred in the main function: name 'DataIngestor' is not defined
"05-Jun-23 16:00:49" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:00:49" - __main__ - INFO - Main function has started...
"05-Jun-23 16:01:02" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:01:12" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:01:12" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:01:13" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:01:13" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:01:56" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:01:56" - __main__ - INFO - Main function has started...
"05-Jun-23 16:02:11" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:02:19" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:02:19" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:02:20" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:02:20" - __main__ - ERROR - An error occurred in the main function: 'DataFrame' object has no attribute 'input_files'
"05-Jun-23 16:07:36" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:07:36" - __main__ - INFO - Main function has started...
"05-Jun-23 16:07:48" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:08:06" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:08:06" - __main__ - INFO - Main function has started...
"05-Jun-23 16:08:18" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:08:18" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:08:18" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:08:19" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:08:19" - __main__ - INFO - Main function has completed.
"05-Jun-23 16:09:38" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:09:38" - __main__ - INFO - Main function has started...
"05-Jun-23 16:09:50" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:09:50" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:09:50" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:09:50" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:09:50" - __main__ - INFO - Main function has completed.
"05-Jun-23 16:17:29" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:17:29" - __main__ - INFO - Main function has started...
"05-Jun-23 16:17:49" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:17:49" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:17:49" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:17:49" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:17:49" - __main__ - INFO - Main function has completed.
"05-Jun-23 16:18:26" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:18:26" - __main__ - INFO - Main function has started...
"05-Jun-23 16:18:38" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:18:38" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:18:38" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:18:38" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:18:38" - __main__ - INFO - Main function has completed.
"05-Jun-23 16:19:37" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:19:37" - __main__ - INFO - Main function has started...
"05-Jun-23 16:19:51" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:19:51" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:19:51" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:19:51" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:19:51" - __main__ - INFO - Main function has completed.
"05-Jun-23 16:20:35" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:20:35" - __main__ - INFO - Main function has started...
"05-Jun-23 16:20:47" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:20:47" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:20:47" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:20:47" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:20:47" - __main__ - INFO - Main function has completed.
"05-Jun-23 16:22:47" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:22:47" - __main__ - INFO - Main function has started...
"05-Jun-23 16:22:59" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:24:30" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:24:30" - __main__ - INFO - Main function has started...
"05-Jun-23 16:24:41" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:24:41" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:24:41" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:24:41" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:24:41" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:25:07" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:25:07" - __main__ - INFO - Main function has started...
"05-Jun-23 16:25:17" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:25:26" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:25:26" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:25:27" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:25:27" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:28:04" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:28:04" - __main__ - INFO - Main function has started...
"05-Jun-23 16:28:17" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:28:17" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:28:17" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:28:17" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:28:17" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:28:44" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:28:44" - __main__ - INFO - Main function has started...
"05-Jun-23 16:28:56" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:29:05" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:29:05" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:29:05" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:29:05" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:30:08" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:30:08" - __main__ - INFO - Main function has started...
"05-Jun-23 16:30:19" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:30:19" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:30:19" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:30:19" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:30:19" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:30:59" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:30:59" - __main__ - INFO - Main function has started...
"05-Jun-23 16:31:10" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:31:18" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:31:18" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:31:19" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:31:22" - __main__ - ERROR - An error occurred in the main function: name 'os' is not defined
"05-Jun-23 16:35:11" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:35:11" - __main__ - INFO - Main function has started...
"05-Jun-23 16:35:22" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:35:30" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:35:30" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:35:31" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:35:34" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'show'
"05-Jun-23 16:42:45" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:42:45" - __main__ - INFO - Main function has started...
"05-Jun-23 16:42:56" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:43:05" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:43:05" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:43:06" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:43:08" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'show'
"05-Jun-23 16:47:40" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:47:40" - __main__ - INFO - Main function has started...
"05-Jun-23 16:47:52" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:48:02" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:48:02" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:48:03" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:48:06" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'show'
"05-Jun-23 16:53:03" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:53:03" - __main__ - INFO - Main function has started...
"05-Jun-23 16:53:17" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:53:30" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:53:30" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:53:32" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:53:35" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'show'
"05-Jun-23 16:53:57" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 16:53:57" - __main__ - INFO - Main function has started...
"05-Jun-23 16:54:12" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:54:12" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:54:12" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 16:54:12" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 16:54:12" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:04:14" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:04:14" - __main__ - INFO - Main function has started...
"05-Jun-23 17:04:29" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:04:38" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:04:38" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:04:41" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:04:43" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'show'
"05-Jun-23 17:11:12" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:11:12" - __main__ - INFO - Main function has started...
"05-Jun-23 17:11:35" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:11:45" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:11:45" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:11:46" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:11:48" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'show'
"05-Jun-23 17:15:37" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:15:37" - __main__ - INFO - Main function has started...
"05-Jun-23 17:15:51" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:15:51" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:15:51" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:15:51" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:15:51" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:16:37" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:16:37" - __main__ - INFO - Main function has started...
"05-Jun-23 17:16:50" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:16:59" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:16:59" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:17:00" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:17:00" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:19:48" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:19:48" - __main__ - INFO - Main function has started...
"05-Jun-23 17:20:00" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:20:09" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:20:09" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:20:10" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:20:10" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:20:54" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:20:54" - __main__ - INFO - Main function has started...
"05-Jun-23 17:21:05" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:21:05" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:21:05" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:21:05" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:21:05" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:21:24" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:21:24" - __main__ - INFO - Main function has started...
"05-Jun-23 17:21:35" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:21:44" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:21:44" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:21:45" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:21:45" - preprocessing - INFO - perform_data_clean() has started...
"05-Jun-23 17:21:45" - preprocessing - ERROR - Error in the method - perform_data_clean(). 'DataFrame' object has no attribute 'city'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\preprocessing.py", line 14, in perform_data_clean_csv
    df_clean = df.select(upper(df.city).alias("city"),
  File "C:\Spark_hadoop\spark-3.2.4-bin-hadoop2.7\python\pyspark\sql\dataframe.py", line 1659, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'city'
"05-Jun-23 17:21:45" - preprocessing - INFO - perform_data_clean() has started...
"05-Jun-23 17:21:45" - preprocessing - ERROR - Error in the method - perform_data_clean(). 'DataFrame' object has no attribute 'city'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\preprocessing.py", line 14, in perform_data_clean_csv
    df_clean = df.select(upper(df.city).alias("city"),
  File "C:\Spark_hadoop\spark-3.2.4-bin-hadoop2.7\python\pyspark\sql\dataframe.py", line 1659, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'city'
"05-Jun-23 17:21:45" - preprocessing - INFO - perform_data_clean_parquet() has started...
"05-Jun-23 17:21:45" - preprocessing - ERROR - Error in the method - perform_data_clean_parquet(). 'DataFrame' object has no attribute 'npi'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\preprocessing.py", line 33, in perform_data_clean_parquet
    df_clean = df.select(df.npi.alias("presc_id"), df.nppes_provider_last_org_name.alias("presc_lname"), \
  File "C:\Spark_hadoop\spark-3.2.4-bin-hadoop2.7\python\pyspark\sql\dataframe.py", line 1659, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'npi'
"05-Jun-23 17:21:45" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:26:19" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:26:19" - __main__ - INFO - Main function has started...
"05-Jun-23 17:26:34" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:26:43" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:26:43" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:26:45" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:26:45" - preprocessing - INFO - perform_data_clean() has started...
"05-Jun-23 17:26:45" - preprocessing - ERROR - Error in the method - perform_data_clean(). 'DataFrame' object has no attribute 'city'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\preprocessing.py", line 14, in perform_data_clean_csv
    df_clean = df.select(upper(df.city).alias("city"),
  File "C:\Spark_hadoop\spark-3.2.4-bin-hadoop2.7\python\pyspark\sql\dataframe.py", line 1659, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'city'
"05-Jun-23 17:26:45" - preprocessing - INFO - perform_data_clean() has started...
"05-Jun-23 17:26:45" - preprocessing - ERROR - Error in the method - perform_data_clean(). 'DataFrame' object has no attribute 'city'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\preprocessing.py", line 14, in perform_data_clean_csv
    df_clean = df.select(upper(df.city).alias("city"),
  File "C:\Spark_hadoop\spark-3.2.4-bin-hadoop2.7\python\pyspark\sql\dataframe.py", line 1659, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'city'
"05-Jun-23 17:26:45" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:31:31" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:31:31" - __main__ - INFO - Main function has started...
"05-Jun-23 17:31:46" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:31:46" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:31:46" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:31:46" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:31:46" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:33:58" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:33:58" - __main__ - INFO - Main function has started...
"05-Jun-23 17:34:11" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:34:11" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:34:11" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:34:11" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:34:11" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:34:59" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:34:59" - __main__ - INFO - Main function has started...
"05-Jun-23 17:35:10" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:35:18" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:35:18" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:35:20" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:35:20" - preprocessing - INFO - perform_data_clean() has started...
"05-Jun-23 17:35:22" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"05-Jun-23 17:35:22" - preprocessing - INFO - perform_data_clean() has completed.
"05-Jun-23 17:35:22" - preprocessing - INFO - perform_data_clean() has started...
"05-Jun-23 17:35:24" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"05-Jun-23 17:35:24" - preprocessing - INFO - perform_data_clean() has completed.
"05-Jun-23 17:35:24" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:47:47" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:47:47" - __main__ - INFO - Main function has started...
"05-Jun-23 17:48:02" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:48:02" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:48:02" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:48:02" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:48:02" - __main__ - ERROR - An error occurred in the main function: perform_data_clean_csv() missing 1 required positional argument: 'df'
"05-Jun-23 17:48:07" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:48:07" - __main__ - INFO - Main function has started...
"05-Jun-23 17:48:19" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:48:19" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:48:19" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:48:19" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:48:19" - __main__ - ERROR - An error occurred in the main function: perform_data_clean_csv() missing 1 required positional argument: 'df'
"05-Jun-23 17:51:09" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:51:09" - __main__ - INFO - Main function has started...
"05-Jun-23 17:51:20" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:51:20" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:51:20" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:51:20" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:51:20" - __main__ - INFO - Main function has completed.
"05-Jun-23 17:51:49" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:51:49" - __main__ - INFO - Main function has started...
"05-Jun-23 17:52:00" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:52:09" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:52:09" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:52:10" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:52:10" - preprocessing - INFO - perform_data_clean() has started...
"05-Jun-23 17:52:10" - preprocessing - ERROR - Error in the method - perform_data_clean(). 'DataFrame' object has no attribute 'city'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\preprocessing.py", line 11, in perform_data_clean_csv
    df_clean = df.select(upper(df.city).alias("city"),
  File "C:\Spark_hadoop\spark-3.2.4-bin-hadoop2.7\python\pyspark\sql\dataframe.py", line 1659, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'city'
"05-Jun-23 17:52:10" - __main__ - ERROR - An error occurred in the main function: 'NoneType' object has no attribute 'rdd'
"05-Jun-23 17:53:39" - __main__ - INFO - Pipeline is Started ...
"05-Jun-23 17:53:39" - __main__ - INFO - Main function has started...
"05-Jun-23 17:53:50" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:53:50" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:53:50" - ingestion - INFO - ingest_data() has started...
"05-Jun-23 17:53:50" - ingestion - INFO - ingest_data() has completed.
"05-Jun-23 17:53:50" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:13:47" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:13:47" - __main__ - INFO - Main function has started...
"06-Jun-23 12:14:07" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:14:07" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:14:07" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:14:07" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:14:07" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:17:15" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:17:15" - __main__ - INFO - Main function has started...
"06-Jun-23 12:17:27" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:17:35" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:17:35" - __main__ - ERROR - An error occurred in the main function: 'list' object has no attribute 'show'
"06-Jun-23 12:22:17" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:22:17" - __main__ - INFO - Main function has started...
"06-Jun-23 12:22:32" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:22:43" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:22:43" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:22:45" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:22:45" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:22:45" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: perform_data_clean_csv() missing 1 required positional argument: 'df1'
"06-Jun-23 12:22:45" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:31:07" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:31:07" - __main__ - INFO - Main function has started...
"06-Jun-23 12:31:19" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:31:27" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:31:27" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:31:29" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:31:29" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:31:29" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:31:29" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:31:29" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:31:30" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:31:30" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:34:50" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:34:50" - __main__ - INFO - Main function has started...
"06-Jun-23 12:35:01" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:35:10" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:35:10" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:35:11" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:35:11" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:35:11" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:35:11" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:35:11" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:35:12" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:35:12" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
"06-Jun-23 12:35:12" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:35:12" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:35:12" - preprocessing - ERROR - Error in the method - perform_data_clean_csv(). 'DataFrame' object has no attribute 'npi'
Traceback (most recent call last):
  File "C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\bin\preprocessing.py", line 11, in perform_data_clean_csv
    df_clean_csv = df.select(df.npi.alias("presc_id"), df.nppes_provider_last_org_name.alias("presc_lname"),
  File "C:\Spark_hadoop\spark-3.2.4-bin-hadoop2.7\python\pyspark\sql\dataframe.py", line 1659, in __getattr__
    raise AttributeError(
AttributeError: 'DataFrame' object has no attribute 'npi'
"06-Jun-23 12:35:12" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:35:12" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:36:31" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:36:31" - __main__ - INFO - Main function has started...
"06-Jun-23 12:36:42" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:36:50" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:36:50" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:36:50" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:36:50" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:36:50" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:36:50" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:36:51" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:36:51" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:36:51" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
"06-Jun-23 12:36:51" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]. Error details: name 'perform_data_clean_parquet' is not defined
"06-Jun-23 12:36:51" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:42:59" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:42:59" - __main__ - INFO - Main function has started...
"06-Jun-23 12:43:10" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:43:18" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:43:18" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:43:19" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:43:19" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:43:19" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:43:19" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:43:19" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:43:20" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:43:20" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
"06-Jun-23 12:43:20" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]. Error details: name 'perform_data_clean_parquet' is not defined
"06-Jun-23 12:43:20" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:45:26" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:45:26" - __main__ - INFO - Main function has started...
"06-Jun-23 12:45:36" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:45:44" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:45:44" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:45:45" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:45:45" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:45:45" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:45:45" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:45:46" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:45:46" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:45:46" - __main__ - INFO - Starting data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]
"06-Jun-23 12:45:46" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]. Error details: name 'perform_data_clean_parquet' is not defined
"06-Jun-23 12:45:46" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:46:54" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:46:54" - __main__ - INFO - Main function has started...
"06-Jun-23 12:47:06" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:47:14" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:47:14" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:47:15" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:47:15" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:47:15" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:47:15" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:47:15" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:47:15" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:47:15" - __main__ - INFO - Starting data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]
"06-Jun-23 12:47:15" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]. Error details: name 'perform_data_clean_parquet' is not defined
"06-Jun-23 12:47:15" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:48:40" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:48:40" - __main__ - INFO - Main function has started...
"06-Jun-23 12:48:51" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:48:59" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:48:59" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:49:00" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:49:00" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:49:00" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:49:00" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:49:00" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:49:00" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:49:00" - __main__ - INFO - Starting data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]
"06-Jun-23 12:49:00" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]. Error details: name 'perform_data_clean_parquet' is not defined
"06-Jun-23 12:49:00" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:54:54" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:54:54" - __main__ - INFO - Main function has started...
"06-Jun-23 12:55:05" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:55:12" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:55:12" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:55:13" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:55:13" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]
"06-Jun-23 12:55:13" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:55:13" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:55:13" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:55:14" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:55:14" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
"06-Jun-23 12:55:14" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]. Error details: name 'perform_data_clean_parquet' is not defined
"06-Jun-23 12:55:14" - __main__ - INFO - Main function has completed.
"06-Jun-23 12:58:09" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 12:58:09" - __main__ - INFO - Main function has started...
"06-Jun-23 12:58:21" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:58:29" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:58:29" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 12:58:30" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 12:58:30" - __main__ - INFO - Starting data cleaning for csv DataFrame
"06-Jun-23 12:58:30" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 12:58:30" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 12:58:30" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:58:31" - __main__ - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 12:58:31" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:58:31" - __main__ - INFO - Starting data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]
"06-Jun-23 12:58:31" - preprocessing - INFO - perform_data_clean_parquet() is started for parquet file...
"06-Jun-23 12:58:31" - preprocessing - INFO - Displaying cleaned parquet DataFrame:
"06-Jun-23 12:58:32" - preprocessing - INFO - perform_data_clean_parquet() has completed.
"06-Jun-23 12:58:32" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 12:58:32" - __main__ - INFO - Main function has completed.
"06-Jun-23 14:24:31" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:24:31" - __main__ - INFO - Main function has started...
"06-Jun-23 14:24:55" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 14:25:10" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 14:25:10" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 14:25:11" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 14:25:11" - __main__ - INFO - Starting data cleaning for csv DataFrame
"06-Jun-23 14:25:11" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 14:25:11" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 14:25:12" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 14:25:13" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 14:25:13" - __main__ - INFO - Starting data cleaning for DataFrame: 
"06-Jun-23 14:25:13" - preprocessing - INFO - perform_data_clean_parquet() is started for parquet file...
"06-Jun-23 14:25:13" - preprocessing - INFO - Displaying cleaned parquet DataFrame:
"06-Jun-23 14:25:14" - preprocessing - INFO - perform_data_clean_parquet() has completed.
"06-Jun-23 14:25:14" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: [DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 14:25:14" - __main__ - INFO - Main function has completed.
"06-Jun-23 14:27:35" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:27:35" - __main__ - INFO - Main function has started...
"06-Jun-23 14:28:00" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 14:28:13" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 14:28:13" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 14:28:14" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 14:28:14" - __main__ - INFO - Starting data cleaning for csv DataFrame
"06-Jun-23 14:28:14" - preprocessing - INFO - perform_data_clean_csv() has started...
"06-Jun-23 14:28:14" - preprocessing - INFO - perform_data_clean_csv() is started for csv file...
"06-Jun-23 14:28:15" - preprocessing - INFO - Displaying cleaned CSV DataFrame:
"06-Jun-23 14:28:15" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 14:28:15" - __main__ - INFO - Starting data cleaning for DataFrame: 
"06-Jun-23 14:28:15" - preprocessing - INFO - perform_data_clean_parquet() is started for parquet file...
"06-Jun-23 14:28:15" - preprocessing - INFO - Displaying cleaned parquet DataFrame:
"06-Jun-23 14:28:16" - preprocessing - INFO - perform_data_clean_parquet() has completed.
"06-Jun-23 14:28:16" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string]. Error details: 'NoneType' object has no attribute 'show'
"06-Jun-23 14:28:16" - __main__ - INFO - Main function has completed.
"06-Jun-23 14:42:54" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:42:54" - __main__ - INFO - Main function has started...
"06-Jun-23 14:43:09" - __main__ - ERROR - An error occurred in the main function: [WinError 3] Le chemin d’accès spécifié est introuvable: 'C:/Users/fabassi/PycharmProjects/pythonProject2/src/main/python/staging/fact\\file1.csv'
"06-Jun-23 14:45:04" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:45:04" - __main__ - INFO - Main function has started...
"06-Jun-23 14:45:14" - __main__ - ERROR - An error occurred in the main function: [WinError 267] Nom de répertoire non valide: 'C:/Users/fabassi/PycharmProjects/pythonProject2/src/main/python/staging/fact\\test.csv'
"06-Jun-23 14:45:55" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:45:55" - __main__ - INFO - Main function has started...
"06-Jun-23 14:46:06" - __main__ - ERROR - An error occurred in the main function: [WinError 267] Nom de répertoire non valide: 'C:/Users/fabassi/PycharmProjects/pythonProject2/src/main/python/staging/fact\\test.csv'
"06-Jun-23 14:49:41" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:49:41" - __main__ - INFO - Main function has started...
"06-Jun-23 14:49:52" - __main__ - ERROR - An error occurred in the main function: [WinError 267] Nom de répertoire non valide: 'C:/Users/fabassi/PycharmProjects/pythonProject2/src/main/python/staging/fact\\test.csv'
"06-Jun-23 14:52:17" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:52:17" - __main__ - INFO - Main function has started...
"06-Jun-23 14:52:28" - __main__ - ERROR - An error occurred in the main function: [WinError 267] Nom de répertoire non valide: 'C:/Users/fabassi/PycharmProjects/pythonProject2/src/main/python/staging//fact\\test.csv'
"06-Jun-23 14:53:56" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:53:56" - __main__ - INFO - Main function has started...
"06-Jun-23 14:54:08" - __main__ - ERROR - An error occurred in the main function: [WinError 267] Nom de répertoire non valide: 'C:\\Users\\fabassi\\PycharmProjects\\pythonProject2\\src\\main\\python\\staging\\fact\\test.csv'
"06-Jun-23 14:55:06" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 14:55:06" - __main__ - INFO - Main function has started...
"06-Jun-23 14:55:17" - __main__ - ERROR - An error occurred in the main function: [WinError 267] Nom de répertoire non valide: 'C:\\Users\\fabassi\\PycharmProjects\\pythonProject2\\src\\main\\python\\staging\\fact\\test.csv.csv'
"06-Jun-23 15:09:33" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 15:09:33" - __main__ - INFO - Main function has started...
"06-Jun-23 15:09:50" - __main__ - ERROR - An error occurred in the main function: module 'Variable' has no attribute 'csv_directory'
"06-Jun-23 15:10:35" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 15:10:35" - __main__ - INFO - Main function has started...
"06-Jun-23 15:10:46" - __main__ - ERROR - An error occurred in the main function: [WinError 267] Nom de répertoire non valide: 'C:/Users/fabassi/PycharmProjects/pythonProject2/src/main/python/staging//fact\\test.csv'
"06-Jun-23 15:13:47" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 15:13:47" - __main__ - INFO - Main function has started...
"06-Jun-23 15:14:12" - __main__ - ERROR - An error occurred in the main function: An error occurred while calling o34.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (FR-PT-117.Jems.local executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\2688978d4fc07c1a34bbdcc8e34d574a\_temporary\0\_temporary\attempt_202306061514127964427053548377074_0001_m_000000_1\part-00000-2254769c-804c-4efe-b696-1d53a983077c-c000.snappy.parquet
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:154)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:300)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)
	... 41 more
Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\fabassi\PycharmProjects\PysparkPorjectOOP\src\python\2688978d4fc07c1a34bbdcc8e34d574a\_temporary\0\_temporary\attempt_202306061514127964427053548377074_0001_m_000000_1\part-00000-2254769c-804c-4efe-b696-1d53a983077c-c000.snappy.parquet
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:154)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:300)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	... 1 more

"06-Jun-23 15:33:47" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 15:33:47" - __main__ - INFO - Main function has started...
"06-Jun-23 15:34:17" - __main__ - ERROR - An error occurred in the main function: path file:/C:/Users/fabassi/PycharmProjects/PysparkPorjectOOP/src/python/2688978d4fc07c1a34bbdcc8e34d574a already exists.
"06-Jun-23 15:35:57" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 15:35:57" - __main__ - INFO - Main function has started...
"06-Jun-23 15:36:09" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 15:36:17" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 15:36:18" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 15:36:19" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 15:36:21" - __main__ - INFO - Starting data cleaning for csv DataFrame
"06-Jun-23 15:36:22" - __main__ - ERROR - An error occurred during data cleaning for DataFrame: DataFrame[npi: string, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: string, total_claim_count: string, total_30_day_fill_count: string, total_day_supply: string, total_drug_cost: string, bene_count_ge65: string, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: string, ge65_suppress_flag: string, total_30_day_fill_count_ge65: string, total_day_supply_ge65: string, total_drug_cost_ge65: string, years_of_exp: string]. Error details: name 'df_fact_sel' is not defined
"06-Jun-23 15:36:22" - __main__ - INFO - Starting data cleaning for DataFrame: 
"06-Jun-23 15:36:22" - __main__ - INFO - Data cleaning completed for DataFrame: 
"06-Jun-23 15:36:22" - __main__ - INFO - Main function has completed.
"06-Jun-23 15:38:50" - __main__ - INFO - Pipeline is Started ...
"06-Jun-23 15:38:50" - __main__ - INFO - Main function has started...
"06-Jun-23 15:39:02" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 15:39:10" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 15:39:11" - ingestion - INFO - ingest_data() has started...
"06-Jun-23 15:39:12" - ingestion - INFO - ingest_data() has completed.
"06-Jun-23 15:39:14" - __main__ - INFO - Starting data cleaning for csv DataFrame
"06-Jun-23 15:39:17" - __main__ - INFO - Data cleaning completed for csv DataFrame
"06-Jun-23 15:39:17" - __main__ - INFO - Starting data cleaning for csv DataFrame
"06-Jun-23 15:39:43" - __main__ - INFO - Data cleaning completed for csv DataFrame
"06-Jun-23 15:39:43" - __main__ - INFO - Starting data cleaning for DataFrame: 
"06-Jun-23 15:39:43" - __main__ - INFO - Data cleaning completed for DataFrame: 
"06-Jun-23 15:39:43" - __main__ - INFO - Main function has completed.
